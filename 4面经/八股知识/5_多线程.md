## Java多线程

![image-20250303215732616](C:\Users\pqy\AppData\Roaming\Typora\typora-user-images\image-20250303215732616.png)

**如果你每次都猜对了**，执行将永远不会停止。如**果你经常猜错**，你会花很多时间停滞，回滚和重新启动。就像火车一样，这将消耗大量的CPU时间周期，比没有猜测效果更差。



### CAS操作

​	**线程在循环中反复的去 CAS 某个变量直到成功为止，通过这种自旋行为，我们可以通过类似锁的效果来实现并发控制，这种锁机制被称为自旋锁**





### Java 使用的线程调度是抢占式的

**抢占式调度（Preemptive Scheduling）**：这种方式存在上下文切换开销，但公平性和 CPU 资源利用率较好，不易阻塞。

**协同式调度（Cooperative Scheduling）**：线程执行完毕后，主动通知系统切换到另一个线程。这种方式可以减少上下文切换带来的性能开销，但公平性较差，容易阻塞



事务有哪些特性？

- **原子性（Atomicity）：事务是不可分割的最小单元，要么全部成功，要么全部失败。**
- **一致性（Consistency）：事务完成时，必须使所有的数据都保持一致状态。**
- **隔离性（Isolation）：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立环境下运行。**
- **持久性（Durability）：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的。**



### 线程的创建

**继承`Thread`类、实现`Runnable`接口、实现`Callable`接口、使用线程池**、



### **死锁**

**四个必要条件**：

1. 互斥条件：该资源任意一个时刻**只由一个线程**占用。
2. 请求与保持条件：一个线程因请求资源而阻塞时，对已获得的资源保持不放。
3. 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。
4. 循环等待条件:若干线程之间形成一种头尾相接的循环等待资源关系。



### 多线程:

### 线程池7大参数:

![image-20250305225829442](C:\Users\pqy\AppData\Roaming\Typora\typora-user-images\image-20250305225829442.png)

**corePoolSize**：核心线程数，**线程池中始终存活的线程数**。

**maximumPoolSize**：最大线程数，线程池中允许的最大线程数，当线程池的任务队列满了之后可以创建的最大线程数。
**keepAliveTime**：指的是空闲线程存活时间。具体说，当线程数大于核心线程数时，空闲线程在等待新任务到达的最大时间，如果超过这个时间还没有任务请求，该空闲线程就会被销毁。

**unit**：空闲线程存活时间的单位。keepAliveTime的计量单位。枚举类型TimeUnit类。

**workQueue**：一个阻塞队列，用来存储线程池等待执行的任务，均为线程安全。



- 时间优先级
- 无界阻塞队列 一进一出

![image-20250311124253711](C:\Users\pqy\AppData\Roaming\Typora\typora-user-images\image-20250311124253711.png)



**threadFactory**：线程工厂，主要用来创建线程



**handler**：拒绝策略，拒绝处理任务时的策略
策略1：ThreadPoolExecutor.AbortPolicy（默认）
在默认的处理策略。该处理在拒绝时抛出RejectedExecutionException，拒绝执行。
策略2：ThreadPoolExecutor.CallerRunsPolicy
调用 execute 方法的线程本身运行任务。这提供了一个简单的反馈控制机制，可以降低新任务提交的速度。将多出来的任务退还给调用者，从而降低流量。
策略3：ThreadPoolExecutor.DiscardOldestPolicy
**抛弃等待队列中等待最久的任务，然后把当前任务加入等待队列。**
策略4：ThreadPoolExecutor.DiscardPolicy
**无法执行的任务被简单地删除**，将会丢弃当前任务，通过源码可以看出，该策略不会执行任务操作。

```java
/**
 * 用给定的初始参数创建一个新的ThreadPoolExecutor。
 */
public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量
                          int maximumPoolSize,//线程池的最大线程数
                          long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间
                          TimeUnit unit,//时间单位
                          BlockingQueue<Runnable> workQueue,//任务队列，用来储存等待执行任务的队列
                          ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可
                          RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务
                           ) {
    if (corePoolSize < 0 ||
        maximumPoolSize <= 0 ||
        maximumPoolSize < corePoolSize ||
        keepAliveTime < 0)
        throw new IllegalArgumentException();
    if (workQueue == null || threadFactory == null || handler == null)
        throw new NullPointerException();
    this.corePoolSize = corePoolSize;
    this.maximumPoolSize = maximumPoolSize;
    this.workQueue = workQueue;
    this.keepAliveTime = unit.toNanos(keepAliveTime);
    this.threadFactory = threadFactory;
    this.handler = handler;
}
```









### 线程池设置

高并发,执行任务时间短,CPU密集型任务尽量使用较小的线程池，**一般为CPU核心数+1**。
因为CPU密集型任务使得CPU使用率很高，若开过多的线程数，只能增加**上下文切换的次数**，因此会带来额外的开销。



执行任务长,IO密集型任务可以使用稍大的线程池，**一般为2*CPU核心数+1**。
因为IO操作不占用CPU，不要让CPU闲下来，**应加大线程数量**，因此可以让CPU在等待IO的时候去处理别的任务，充分利用CPU时间。

![image-20250311133459370](C:\Users\pqy\AppData\Roaming\Typora\typora-user-images\image-20250311133459370.png)





gpu关心吞吐量,并行度;增加线程数量来优化延迟问题,提供并行能力;**线程往往超配**

cpu更加关心延迟和并发;减少延迟时间优化执行效率;







### 协程

是一种用户级的轻量级线程。协程拥有自己的寄存器上下文和栈

协程运行在线程之上，当一个协程执行完成后，可以选择主动让出，让另一个协程运行在当前线程之上。协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程，而且协程的切换在用户态完成，切换的代价比线程从用户态到内核态的代价小很多



### 线程如何运行

继承 Thread 类或实现 Runnable 接口来



### 乐观锁和悲观锁

#### 什么是悲观锁？

​	悲观锁总是假设最坏的情况，认为共享资源每次被访问的时候就会出现问题(比如共享数据被修改)，所以每次在获取资源操作的时候都会上锁，这样其他线程想拿到这个资源就会阻塞直到锁被上一个持有者释放。也就是说，**共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程**。

​	高并发的场景下，**激烈的锁竞争会造成线程阻塞**，大量阻塞线程会导致系统的上下文切换，增加系统的性能开销。并且，**悲观锁还可能会存在死锁问题，影响代码的正常运行**。



#### 什么是乐观锁？

​	**乐观锁总是假设最好的情况，认为共享资源每次被访问的时候不会出现问题，线程可以不停地执行，无需加锁也无需等待，只是在提交修改的时候去验证对应的资源（也就是数据）是否被其它线程修改了**（具体方法可以使用版本号机制或 CAS 算法）。

​	在 Java 中`java.util.concurrent.atomic`包下面的原子变量类（比如`AtomicInteger`、`LongAdder`）就是使用了乐观锁的一种实现方式 **CAS** 实现的。



#### 对比

悲观锁适合写操作多的场景，先加锁可以保证**写操作**时数据正确。
乐观锁适合读操作多的场景，不加锁的特点能够使其**读操作**的性能大幅提升。







### Synchronized

​	底层使用指令码方式来控制锁的，映射成字节码指令就是增加来两个指令：monitorenter和monitorexit。当线程执行遇到monitorenter指令时会尝试获取内置锁，如果获取锁则锁计数器+1，如果没有获取锁则阻塞；当遇到monitorexit指令时锁计数器-1，如果计数器为0则释放锁。



### Lock

​	**底层实现**是是CAS乐观锁，依赖AbstractQueuedSynchronizer类，把所有的请求线程构成一个CLH队列。而对该队列的操作均通过Lock-Free（CAS）操作。

​	synchronized原始采用的是**CPU悲观锁**机制，即线程获得的是独占锁, 独占锁意味着其他线程**只能依靠阻塞**来等待线程释放锁,而在CPU转换线程阻塞时会引起线程上下文切换，当有很多线程竞争锁的时候，会引起CPU频繁的上下文切换导致效率很低。



​	而Lock用的是**乐观锁**方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。**乐观锁**实现的机制就是**CAS操作**（Compare and Swap）。==//?????==

​	锁在空闲的才能获取锁（未获得锁不会等待）。举个例子：当两个线程同时通过lock.trylock()想获取某个锁时，假若此时线程A获取到了锁，而线程B不会等待，直接放弃获取锁



### Lock到底是乐观还是悲观锁







//3

### Synchronized和Lock对比



**我的:**

相同:

- 都是**可重入**的锁
- 都是**悲观锁**思想



不同:

- 一个是关键字由于内置语言实现,一个是接口

- 都是**可重入**,但涉及**锁的细粒度控制**

  ​	当需要对资源的访问进行更细粒度的控制时，`ReentrantLock` 比 `synchronized` 提供了更高的灵活性。**例如，在一个大的方法中，只有少数几行代码需要同步，使用 `ReentrantLock` 可以只在这几行代码周围加锁和解锁**，从而减少锁持有的时间，提高效率,但同样Lock异常时不会自动释放锁，所以**需要在finally中实现释放锁**。

- trylock **非阻塞的竞争锁**方法

​		锁在空闲的才能获取锁（未获得锁不会等待）。举个例子：当两个线程同时通过lock.trylock()想获取某个锁时，假若此时线程A获取到了锁，而线程B不会等待，**直接放弃获取锁**。

-  lock有**公平和非公平锁**两种机制

​		**公平锁** : 锁被释放之后，先申请的线程先得到锁。**性能较差**一些，因为公平锁为了保证时间上的绝对顺序，上下文切换更频繁。

​		**非公平锁**：锁被释放之后，后申请的线程可能会先获取到锁，是随机或者按照其他优先级排序的。性能更好，**但可能会导致某些线程永远无法获取到锁**。



### Java中“Reader-Writer”锁和“ReentrantReadWriteLock”之间的区别

哪个使用起来更灵活？
读写锁：
	读写锁允许**多个线程同时读取共享资源**，**但一次只有一个线程可以写入**。当一个线程想要写入资源时，它必须等待所有读取者完成读取才能获取锁。
	读写锁是不可重入的，这意味着持有读锁的线程在不释放读锁的情况下无法获取写锁。类似地，持有写锁的线程在不释放写锁的情况下无法获取读锁。



**可重入读写锁**：

- ReentrantReadWriteLock 是读写锁的更灵活的实现。它允许**多个线程同时获取读锁，也允许持有读锁的线程无需先释放读锁即可获取写锁**。这使得线程可以将读锁升级为写锁。

- ReentrantReadWriteLock是**可重入**的，这意味着持有锁进行读或写的线程可以再次获取锁，而无需先释放锁。

  总的来说，可重入读写锁比读写锁提供了更多的灵活性，但它也更复杂，如果使用不当，可能会导致死锁。当需要对锁进行**更细粒度**的控制时，通常建议使用 ReentrantReadWriteLock，而当需要简单性时，建议使用 Reader-Writer 锁。















### 重⼊锁、读写锁、阻塞队列以及原⼦并发类



### synchronized

`synchronized`只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。



### ReentrantLock

​	`ReentrantLock` 实现了 `Lock` 接口，是一个**可重入且独占式**的锁，和 `synchronized` 关键字类似。不过，`ReentrantLock` 更灵活、更强大，增加了轮询、超时、中断、公平锁和非公平锁等高级功能。

`ReentrantLock` 默认使用**非公平锁**，也可以通过构造器来显式的指定使用公平锁。



### 公平锁和非公平锁有什么区别？

- **公平锁** : 锁被释放之后，先申请的线程先得到锁。性能较差一些，因为公平锁为了保证时间上的绝对顺序，上下文切换更频繁。
- **非公平锁**：锁被释放之后，后申请的线程可能会先获取到锁，是随机或者按照其他优先级排序的。性能更好，但可能会导致某些线程永远无法获取到锁。

​	公平锁的实现需要**维护一个等待队列**，记录等待锁资源的线程，并按照请求顺序分配锁。这可能会增加一些开销，包括锁的管理和线程调度的开销。在高并发情况下，公平锁的性能可能相对较低，但可以提供公平性和可预测性的保证。

公平锁适用于对锁资源的分配有严格要求的场景，例如资源竞争激烈、对各个线程的公平性要求较高的情况。

12306中要解决买票严格按照顺序执行，就需要使用公平锁分配锁，确保较早请求的线程先获得锁。







### synchronized 和 ReentrantLock 有什么区别？





#### synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API

​		`synchronized` 是依赖于 JVM 实现的，前面我们也讲到了 虚拟机团队在 JDK1.6 为 `synchronized` 关键字进行了很多优化，但是这些优化都是在虚拟机层面实现的，并没有直接暴露给我们。

`		ReentrantLock` 是 JDK 层面实现的（也就是 API 层面，需要 lock() 和 unlock() 方法配合 try/finally 语句块来完成），所以我们可以通过查看它的源代码，来看它是如何实现的。



#### ReentrantLock 比 synchronized 增加了一些高级功能

相比`synchronized`，`ReentrantLock`增加了一些高级功能。主要来说主要有三点：

- **等待可中断** : `ReentrantLock`提供了一种能够**中断等待锁的线程的机制**，通过 `lock.lockInterruptibly()` 来实现这个机制。也就是说**当前线程在等待获取锁的过程中**，如果其他线程**中断当前线程**「 `interrupt()` 」，当前线程就会抛出 `InterruptedException` 异常，可以捕捉该异常进行相应处理。
- **可实现公平锁** : `ReentrantLock`可以**指定是公平锁还是非公平锁**。而`synchronized`只能是非公平锁。所谓的公平锁就是先等待的线程先获得锁。`ReentrantLock`默认情况是非公平的，可以通过 `ReentrantLock`类的`ReentrantLock(boolean fair)`构造方法来指定是否是公平的。
- **支持超时** ：`ReentrantLock` 提供了 `tryLock(timeout)` 的方法，可以指定等待获取锁的最长等待时间，如果超过了等待时间，就会获取锁失败，不会一直等待。

如果你想使用上述功能，那么选择 `ReentrantLock` 是一个不错的选择









ReentrantLock 只是 Lock 接口的一个实现而已。

Re-Entrant-Lock：**即表示可重新反复进入的锁，但仅限于当前线程**；

释放锁操作必须在 **finally** 里面，不然如果出现异常导致锁不能被正常释放，进而会卡死后续所有访问该锁的别的线程

**优点使用场景:**

背景:线程1需要临界区A，同时A需要临界区B，临界区B需要获取被A持有的锁，临界区A需要临界区B的资源，**引发死锁**

**锁的细粒度控制**

​	当需要对资源的访问进行更细粒度的控制时，`ReentrantLock` 比 `synchronized` 提供了更高的灵活性。**例如，在一个大的方法中，只有少数几行代码需要同步，使用 `ReentrantLock` 可以只在这几行代码周围加锁和解锁**，从而减少锁持有的时间，提高效率







**Redis锁?**

非公平锁（Non-Fair Lock）是一种锁的获取策略，它允许在锁释放时，新的请求可以插队获取锁，而不必按照请求的顺序等待。相对而言，公平锁（Fair Lock）则按照请求的顺序分配锁，确保较早请求的线程先获得锁。

